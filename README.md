# ðŸ§  Local RAG Chatbot with Ollama + Chroma + Streamlit

This project implements a **Retrieval-Augmented Generation (RAG) pipeline** running fully **locally**, using:

- [Ollama](https://ollama.ai) â†’ to run LLMs like `llama3`.  
- [LangChain](https://www.langchain.com) â†’ orchestration for RAG.  
- [Chroma](https://www.trychroma.com) â†’ local vector database for document embeddings.  
- [Streamlit](https://streamlit.io) â†’ simple interactive UI.  

---

## ðŸš€ Features
- use documents (csv).  
- Store embeddings in **Chroma** (persisted locally).  
- Run queries against your knowledge base.  
- Responses are generated using **Ollama LLMs**, fully offline.  
- Caches embeddings so repeated runs are faster.  

---

## ðŸ“‚ Project Structure

Learning Model Locally/
â”‚â”€â”€ main.py # Streamlit app entrypoint
â”‚â”€â”€ db_utils.py # Database utils (Chroma vector store)
â”‚â”€â”€ qa_utils.py # Query + RAG pipeline
â”‚â”€â”€ requirements.txt # Python dependencies
â”‚â”€â”€ README.md # Project documentation
â””â”€â”€ data/ # Folder for storing uploaded files