{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f79adb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output:\n",
      " It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "payload = {\n",
    "    \"model\": \"llama3.1:8b\",   \n",
    "    \"prompt\": \"Hi\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, stream=True)\n",
    "\n",
    "# print(response.text)\n",
    "output_text = \"\"\n",
    "for line in response.iter_lines():\n",
    "    if line:\n",
    "        data = json.loads(line.decode(\"utf-8\"))\n",
    "        if \"response\" in data:\n",
    "            output_text += data[\"response\"]\n",
    "        if data.get(\"done\", False): \n",
    "            break\n",
    "\n",
    "print(\"Final Output:\\n\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6117430e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec5425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feddb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853f52e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56a914da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will work on vectore store database\n",
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv('./data/realistic_restaurant_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98be64a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df380da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_in_chroma(df):\n",
    "    if \"Review\" not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain a 'Review' column\")\n",
    "    \n",
    "    texts = df[\"Review\"].dropna().astype(str).tolist()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    documents = text_splitter.create_documents(texts)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    print('EMbdi',embeddings)\n",
    "    persist_dir = \"./chroma_db_sent\"\n",
    "    os.makedirs(persist_dir, exist_ok=True)\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=\"collection\",\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=persist_dir\n",
    "    )\n",
    "\n",
    "    existing_docs = vectorstore.get()\n",
    "    existing_texts = set(existing_docs[\"documents\"]) if existing_docs else set()\n",
    "    prev_count = len(existing_texts)\n",
    "    new_texts = [doc.page_content for doc in documents if doc.page_content not in existing_texts]\n",
    "    if new_texts:\n",
    "        vectorstore.add_texts(new_texts)\n",
    "        vectorstore.persist()\n",
    "        print(f\"Added {len(new_texts)} new chunks to ChromaDB\")\n",
    "    else:\n",
    "        print(\"No new data to add, everything already exists in ChromaDB\")\n",
    "\n",
    "    updated_docs = vectorstore.get()\n",
    "    total_count = len(updated_docs[\"documents\"]) if updated_docs else 0\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad50547",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vect_data\u001b[38;5;241m=\u001b[39mstore_in_chroma(\u001b[43mdata\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "vect_data=store_in_chroma(data)\n",
    "print(vect_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7dbb792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.vectorstores.chroma.Chroma object at 0x000001386B06A210>\n"
     ]
    }
   ],
   "source": [
    "print(vect_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a93b0eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retriever():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=\"collection\",   \n",
    "        persist_directory=\"./chroma_db_sent\",\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",   \n",
    "        search_kwargs={\"k\": 3}    \n",
    "    )\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c186b4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='90-minute wait on a Thursday night with no option to call ahead. The pizza was good when we finally got it, but nothing special enough to justify such a long wait. Many comparable options with better systems.'), Document(page_content=\"Nothing terrible but nothing special either. The crust was okay, toppings were standard, and service was fine. It's the kind of place you go when you're in the area, but wouldn't make a special trip for.\"), Document(page_content=\"Ordered for Valentine's Day and regretted it. Pizza took 2 hours to arrive, was cold, and had the wrong toppings. Called the restaurant and they seemed completely unconcerned. Never again.\")]\n"
     ]
    }
   ],
   "source": [
    "retriever = get_retriever()\n",
    "docs = retriever.get_relevant_documents(\"Disappointed with service ?\")\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9d754da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Title        Date  Rating  \\\n",
      "0            Best pizza in town  2024-03-15       5   \n",
      "1     Disappointed with service  2024-02-20       2   \n",
      "2  Authentic Italian experience  2024-01-05       5   \n",
      "3   Overpriced for what you get  2024-03-01       3   \n",
      "4      Great gluten-free option  2024-02-15       4   \n",
      "\n",
      "                                              Review  \n",
      "0  The crust was perfectly crispy on the outside ...  \n",
      "1  While the pizza itself was decent, we waited o...  \n",
      "2  This place reminds me of the pizzerias in Napl...  \n",
      "3  $24 for a medium pizza with just two toppings ...  \n",
      "4  As someone with celiac disease, finding good g...  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())   # should be > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7983a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama3.1:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e6986b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ddurande\\AppData\\Local\\Temp\\ipykernel_10032\\2708740973.py:33: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  result = qa_chain.run({\"query\": query})\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "QA_TEMPLATE = \"\"\"\n",
    "You are an expert about answering questions about a pizza restaurant.\n",
    "Always give clear, helpful, and concise answers.\n",
    "\n",
    "Use the following pieces of retrieved context to answer the user’s question.\n",
    "If you don’t know the answer,try to get it from your knowledge and append  this sentence ,\"i'm not sure about this\" ,don’t make things up.\n",
    "And make sure answer must be in 2 to 3 lines.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=QA_TEMPLATE,\n",
    ")\n",
    "\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever, \n",
    "    chain_type=\"stuff\", #For converting context into prompt \n",
    "    chain_type_kwargs={\"prompt\": qa_prompt}\n",
    ")\n",
    "\n",
    "\n",
    "query = \"Late night savior?\"\n",
    "result = qa_chain.run({\"query\": query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8bc3cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This pizza place has been a late-night lifesaver, open until 3am on weekends. They've got huge slices that are reheated to perfection and creative special menu items that hit the spot when you're hungry late at night. Perfect for those long nights out!\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4e9cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_fallback = Ollama(model=\"llama3.1:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7523538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.llms import Ollama\n",
    "# this will work if the question is outsider or which is not releated to the document\n",
    "def process_with_fallback(result: str, query: str) -> str:\n",
    "    irrelevant_markers = [\n",
    "        \"doesn't seem to be about\",\n",
    "        \"this conversation doesn't seem\",\n",
    "        \"i'd be happy to try and help\",\n",
    "        \"not related\",\n",
    "        \"i'm not sure about this\",\n",
    "        \"this question is not relevant\",\n",
    "        \"out of context\"\n",
    "    ]\n",
    "    if any(marker.lower() in result.lower() for marker in irrelevant_markers):\n",
    "        # print(\"Irrelevant result detected, switching to fallback LLM...\")\n",
    "        prompt = f\"\"\"\n",
    "            Please answer the following question in only 2–3 concise lines:\n",
    "\n",
    "            Question: {query}\n",
    "            \"\"\"\n",
    "        fallback_answer = llm_fallback.invoke(prompt)\n",
    "        return fallback_answer\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bba7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, they fit the bill as a late-night savior with their creative and delicious menu items. Their special late-night options are perfect for hitting the spot when hunger strikes in the middle of the night.\n"
     ]
    }
   ],
   "source": [
    "print(process_with_fallback(result,query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f852398b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
